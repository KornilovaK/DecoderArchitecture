{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c84e4470-27b0-4666-9b83-5b238e89a11b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d0cc3f9-0c2d-4e7d-bae4-19132103b9d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import math\n",
    "import json\n",
    "import pickle\n",
    "from dataclasses import dataclass\n",
    "import wandb\n",
    "\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "from torch.amp import autocast, GradScaler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from reflex_model import GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9dd4553-914b-4969-8363-b4cb98cced3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch._dynamo\n",
    "torch._dynamo.config.suppress_errors = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c4904d-19b0-4d63-b496-8bc531e08e79",
   "metadata": {},
   "source": [
    "# init model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "91a5eb4e-d5b5-4c74-9271-307ddee0761d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class GPTConfig:\n",
    "    batch_size: int = 8\n",
    "    block_size: int = 2048\n",
    "    vocab_size: int = 50257\n",
    "    n_layer: int = 6\n",
    "    n_head: int = 8\n",
    "    n_embd: int = 1536\n",
    "    dropout: float = 0.1\n",
    "    bias: bool = True\n",
    "    pretrained_model_path: str=\"/home/user/models/rugpt\"\n",
    "    local_files_only: bool=True\n",
    "    \n",
    "config = GPTConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12908e9e-7991-422b-9477-dd2ff0f0f6cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb10d596-8225-4ec5-a5e6-22563e99f148",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = json.load(open('/home/user/DecoderArchitecture/run_params.json'))\n",
    "params['wandb_run_name'] = f\"KornilovaK-{params['wandb_project']}\"\n",
    "params['dtype'] = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16'\n",
    "params['gradient_accumulation_steps'] = 2\n",
    "os.makedirs(params['out_dir'], exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5f49362b-5ae8-4f08-9dea-f4b9a40b0445",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens per iteration will be: 32,768\n"
     ]
    }
   ],
   "source": [
    "tokens_per_iter = params['gradient_accumulation_steps'] * params['ddp_world_size'] * config.batch_size * config.block_size\n",
    "print(f\"tokens per iteration will be: {tokens_per_iter:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "94f22c41-7139-42cf-a496-79262beefba6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(torch.backends.cuda.flash_sdp_enabled())\n",
    "print(torch.backends.cuda.mem_efficient_sdp_enabled())\n",
    "print(torch.backends.cuda.math_sdp_enabled())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c2fe98c1-2878-4ac4-9620-c672443e1234",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1337 + params['seed_offset'])\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "device_type = params['device']\n",
    "\n",
    "ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[params['dtype']]\n",
    "ctx = autocast(device_type=device_type, dtype=ptdtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "101f57d4-abcb-46a9-b93b-64403fdd29a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_args = dict(\n",
    "    n_layer=config.n_layer,\n",
    "    n_head=config.n_head,\n",
    "    n_embd=config.n_embd,\n",
    "    block_size=config.block_size,\n",
    "    bias=config.bias,\n",
    "    vocab_size=config.vocab_size,\n",
    "    dropout=config.dropout\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f8beaa3d-dee9-4dd0-95c1-59a40ba2e5e1",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copied: transformer.wte.weight -> transformer.wte.weight\n",
      "Copied: lm_head.weight -> lm_head.weight\n",
      "Copied: transformer.wpe.weight -> transformer.wpe.weight\n",
      "Copied: transformer.ln_f.weight -> transformer.ln_f.weight\n",
      "Copied: transformer.ln_f.bias -> transformer.ln_f.bias\n",
      "Copied: transformer.h.0.ln_1.weight -> transformer.h.0.ln_1.weight\n",
      "Copied: transformer.h.0.ln_1.bias -> transformer.h.0.ln_1.bias\n",
      "Copied: transformer.h.0.ln_2.weight -> transformer.h.0.ln_2.weight\n",
      "Copied: transformer.h.0.ln_2.bias -> transformer.h.0.ln_2.bias\n",
      "Copied: transformer.h.0.attn.c_attn.weight -> transformer.h.0.attn.c_attn.weight\n",
      "Copied: transformer.h.0.attn.c_attn.bias -> transformer.h.0.attn.c_attn.bias\n",
      "Copied: transformer.h.0.attn.c_proj.weight -> transformer.h.0.attn.c_proj.weight\n",
      "Copied: transformer.h.0.attn.c_proj.bias -> transformer.h.0.attn.c_proj.bias\n",
      "Copied: transformer.h.0.mlp.c_fc.weight -> transformer.h.0.mlp.c_fc.weight\n",
      "Copied: transformer.h.0.mlp.c_fc.bias -> transformer.h.0.mlp.c_fc.bias\n",
      "Copied: transformer.h.0.mlp.c_proj.weight -> transformer.h.0.mlp.c_proj.weight\n",
      "Copied: transformer.h.0.mlp.c_proj.bias -> transformer.h.0.mlp.c_proj.bias\n",
      "Copied: transformer.h.1.ln_1.weight -> transformer.h.1.ln_1.weight\n",
      "Copied: transformer.h.1.ln_1.bias -> transformer.h.1.ln_1.bias\n",
      "Copied: transformer.h.1.ln_2.weight -> transformer.h.1.ln_2.weight\n",
      "Copied: transformer.h.1.ln_2.bias -> transformer.h.1.ln_2.bias\n",
      "Copied: transformer.h.1.attn.c_attn.weight -> transformer.h.1.attn.c_attn.weight\n",
      "Copied: transformer.h.1.attn.c_attn.bias -> transformer.h.1.attn.c_attn.bias\n",
      "Copied: transformer.h.1.attn.c_proj.weight -> transformer.h.1.attn.c_proj.weight\n",
      "Copied: transformer.h.1.attn.c_proj.bias -> transformer.h.1.attn.c_proj.bias\n",
      "Copied: transformer.h.1.mlp.c_fc.weight -> transformer.h.1.mlp.c_fc.weight\n",
      "Copied: transformer.h.1.mlp.c_fc.bias -> transformer.h.1.mlp.c_fc.bias\n",
      "Copied: transformer.h.1.mlp.c_proj.weight -> transformer.h.1.mlp.c_proj.weight\n",
      "Copied: transformer.h.1.mlp.c_proj.bias -> transformer.h.1.mlp.c_proj.bias\n",
      "Copied: transformer.h.2.ln_1.weight -> transformer.h.2.ln_1.weight\n",
      "Copied: transformer.h.2.ln_1.bias -> transformer.h.2.ln_1.bias\n",
      "Copied: transformer.h.2.ln_2.weight -> transformer.h.2.ln_2.weight\n",
      "Copied: transformer.h.2.ln_2.bias -> transformer.h.2.ln_2.bias\n",
      "Copied: transformer.h.2.attn.c_attn.weight -> transformer.h.2.attn.c_attn.weight\n",
      "Copied: transformer.h.2.attn.c_attn.bias -> transformer.h.2.attn.c_attn.bias\n",
      "Copied: transformer.h.2.attn.c_proj.weight -> transformer.h.2.attn.c_proj.weight\n",
      "Copied: transformer.h.2.attn.c_proj.bias -> transformer.h.2.attn.c_proj.bias\n",
      "Copied: transformer.h.2.mlp.c_fc.weight -> transformer.h.2.mlp.c_fc.weight\n",
      "Copied: transformer.h.2.mlp.c_fc.bias -> transformer.h.2.mlp.c_fc.bias\n",
      "Copied: transformer.h.2.mlp.c_proj.weight -> transformer.h.2.mlp.c_proj.weight\n",
      "Copied: transformer.h.2.mlp.c_proj.bias -> transformer.h.2.mlp.c_proj.bias\n",
      "Copied: transformer.h.3.ln_1.weight -> transformer.h.3.ln_1.weight\n",
      "Copied: transformer.h.3.ln_1.bias -> transformer.h.3.ln_1.bias\n",
      "Copied: transformer.h.3.ln_2.weight -> transformer.h.3.ln_2.weight\n",
      "Copied: transformer.h.3.ln_2.bias -> transformer.h.3.ln_2.bias\n",
      "Copied: transformer.h.3.attn.c_attn.weight -> transformer.h.3.attn.c_attn.weight\n",
      "Copied: transformer.h.3.attn.c_attn.bias -> transformer.h.3.attn.c_attn.bias\n",
      "Copied: transformer.h.3.attn.c_proj.weight -> transformer.h.3.attn.c_proj.weight\n",
      "Copied: transformer.h.3.attn.c_proj.bias -> transformer.h.3.attn.c_proj.bias\n",
      "Copied: transformer.h.3.mlp.c_fc.weight -> transformer.h.3.mlp.c_fc.weight\n",
      "Copied: transformer.h.3.mlp.c_fc.bias -> transformer.h.3.mlp.c_fc.bias\n",
      "Copied: transformer.h.3.mlp.c_proj.weight -> transformer.h.3.mlp.c_proj.weight\n",
      "Copied: transformer.h.3.mlp.c_proj.bias -> transformer.h.3.mlp.c_proj.bias\n",
      "Copied: transformer.h.4.ln_1.weight -> transformer.h.4.ln_1.weight\n",
      "Copied: transformer.h.4.ln_1.bias -> transformer.h.4.ln_1.bias\n",
      "Copied: transformer.h.4.ln_2.weight -> transformer.h.4.ln_2.weight\n",
      "Copied: transformer.h.4.ln_2.bias -> transformer.h.4.ln_2.bias\n",
      "Copied: transformer.h.4.attn.c_attn.weight -> transformer.h.4.attn.c_attn.weight\n",
      "Copied: transformer.h.4.attn.c_attn.bias -> transformer.h.4.attn.c_attn.bias\n",
      "Copied: transformer.h.4.attn.c_proj.weight -> transformer.h.4.attn.c_proj.weight\n",
      "Copied: transformer.h.4.attn.c_proj.bias -> transformer.h.4.attn.c_proj.bias\n",
      "Copied: transformer.h.4.mlp.c_fc.weight -> transformer.h.4.mlp.c_fc.weight\n",
      "Copied: transformer.h.4.mlp.c_fc.bias -> transformer.h.4.mlp.c_fc.bias\n",
      "Copied: transformer.h.4.mlp.c_proj.weight -> transformer.h.4.mlp.c_proj.weight\n",
      "Copied: transformer.h.4.mlp.c_proj.bias -> transformer.h.4.mlp.c_proj.bias\n",
      "Copied: transformer.h.5.ln_1.weight -> transformer.h.5.ln_1.weight\n",
      "Copied: transformer.h.5.ln_1.bias -> transformer.h.5.ln_1.bias\n",
      "Copied: transformer.h.5.ln_2.weight -> transformer.h.5.ln_2.weight\n",
      "Copied: transformer.h.5.ln_2.bias -> transformer.h.5.ln_2.bias\n",
      "Copied: transformer.h.5.attn.c_attn.weight -> transformer.h.5.attn.c_attn.weight\n",
      "Copied: transformer.h.5.attn.c_attn.bias -> transformer.h.5.attn.c_attn.bias\n",
      "Copied: transformer.h.5.attn.c_proj.weight -> transformer.h.5.attn.c_proj.weight\n",
      "Copied: transformer.h.5.attn.c_proj.bias -> transformer.h.5.attn.c_proj.bias\n",
      "Copied: transformer.h.5.mlp.c_fc.weight -> transformer.h.5.mlp.c_fc.weight\n",
      "Copied: transformer.h.5.mlp.c_fc.bias -> transformer.h.5.mlp.c_fc.bias\n",
      "Copied: transformer.h.5.mlp.c_proj.weight -> transformer.h.5.mlp.c_proj.weight\n",
      "Copied: transformer.h.5.mlp.c_proj.bias -> transformer.h.5.mlp.c_proj.bias\n",
      "Initializing new parameter: transformer.h.0.mlp.act.proj.weight\n",
      "Initializing new parameter: transformer.h.0.mlp.act.proj.bias\n",
      "Initializing new parameter: transformer.h.1.mlp.act.proj.weight\n",
      "Initializing new parameter: transformer.h.1.mlp.act.proj.bias\n",
      "Initializing new parameter: transformer.h.2.mlp.act.proj.weight\n",
      "Initializing new parameter: transformer.h.2.mlp.act.proj.bias\n",
      "Initializing new parameter: transformer.h.3.mlp.act.proj.weight\n",
      "Initializing new parameter: transformer.h.3.mlp.act.proj.bias\n",
      "Initializing new parameter: transformer.h.4.mlp.act.proj.weight\n",
      "Initializing new parameter: transformer.h.4.mlp.act.proj.bias\n",
      "Initializing new parameter: transformer.h.5.mlp.act.proj.weight\n",
      "Initializing new parameter: transformer.h.5.mlp.act.proj.bias\n"
     ]
    }
   ],
   "source": [
    "scaler = GradScaler()\n",
    "model = GPT(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "196b4229-2b1a-413b-b2c9-251475fbfe12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "est checkpoint size: 8.40 GB\n",
      "700.2 millions of params\n"
     ]
    }
   ],
   "source": [
    "params_total = model.get_num_params()\n",
    "params_bytes = params_total*4\n",
    "params_and_buffers_bytes = params_bytes + 2*params_bytes\n",
    "print(f\"est checkpoint size: {params_and_buffers_bytes/1e9:.2f} GB\")\n",
    "print(f\"{params_total/1e6:.1f} millions of params\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9a493cb5-72e0-446d-bcc5-c503295bd897",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num decayed parameter tensors: 32, with 703,194,624 parameters\n",
      "num non-decayed parameter tensors: 56, with 196,608 parameters\n",
      "using fused AdamW: True\n"
     ]
    }
   ],
   "source": [
    "optimizer = model.configure_optimizers(params['weight_decay'], params['learning_rate'], (params['beta1'], params['beta2']), device_type)\n",
    "checkpoint = None\n",
    "\n",
    "model = torch.compile(model.to(params['device']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade4bc52-8b3f-4c1c-89ee-deaa77f37fdf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "77ab4593-672b-4ef6-97fc-771e379f2254",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# init run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9ddee2a4-4203-4079-a118-c141bacc64e6",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Failed to detect the name of this notebook. You can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkornilova_eka\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.20.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/user/DecoderArchitecture/wandb/run-20250617_234743-4ujz74ec</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/kornilova_eka/reflex_attention/runs/4ujz74ec' target=\"_blank\">KornilovaK-reflex_attention</a></strong> to <a href='https://wandb.ai/kornilova_eka/reflex_attention' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/kornilova_eka/reflex_attention' target=\"_blank\">https://wandb.ai/kornilova_eka/reflex_attention</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/kornilova_eka/reflex_attention/runs/4ujz74ec' target=\"_blank\">https://wandb.ai/kornilova_eka/reflex_attention/runs/4ujz74ec</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/kornilova_eka/reflex_attention/runs/4ujz74ec?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f71cf7f6650>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(project=params['wandb_project'], name=params['wandb_run_name'], config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7c8430ca-c7b1-4998-bac3-5169f9b04ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size                    = config.block_size\n",
    "batch_size                    = config.batch_size\n",
    "data_dir                      = params['data_dir']\n",
    "learning_rate                 = params['learning_rate']\n",
    "warmup_iters                  = params['warmup_iters']\n",
    "out_dir                       = params['out_dir']\n",
    "log_interval                  = params['log_interval']\n",
    "max_iters                     = params['max_iters']\n",
    "gradient_accumulation_steps   = params['gradient_accumulation_steps']\n",
    "grad_clip                     = params['grad_clip']\n",
    "eval_interval                 = params['eval_interval']\n",
    "eval_iters                    = params['eval_iters']\n",
    "min_lr                        = params['min_lr']\n",
    "lr_decay_iters                = params['lr_decay_iters']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a36ae0-0e25-42b4-b2e0-c305be2ca5c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "41c05db8-8609-4130-88f1-06eac70309ca",
   "metadata": {},
   "source": [
    "# run training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9291b651-21f8-4f19-bb75-4afede72a7e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lr(it):\n",
    "    if it < warmup_iters:\n",
    "        return learning_rate * it / warmup_iters\n",
    "    if it > lr_decay_iters:\n",
    "        return min_lr\n",
    "        \n",
    "    decay_ratio = (it - warmup_iters) / (lr_decay_iters - warmup_iters)\n",
    "    assert 0 <= decay_ratio <= 1\n",
    "    \n",
    "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio))\n",
    "    return min_lr + coeff * (learning_rate - min_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a4fe7cec-4765-40c0-af02-68a6aca9faa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(split):\n",
    "    if split == 'train':\n",
    "        data = np.memmap(os.path.join(data_dir, 'train.bin'), dtype=np.uint16, mode='r')\n",
    "    else:\n",
    "        data = np.memmap(os.path.join(data_dir, 'val.bin'), dtype=np.uint16, mode='r')\n",
    "        \n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([torch.from_numpy((data[i:i+block_size]).astype(np.int64)) for i in ix])\n",
    "    y = torch.stack([torch.from_numpy((data[i+1:i+1+block_size]).astype(np.int64)) for i in ix])\n",
    "    \n",
    "    assert device_type == 'cuda'\n",
    "    x, y = x.pin_memory().to(device_type, non_blocking=True), y.pin_memory().to(device_type, non_blocking=True)\n",
    "\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "36e5b9d9-6f55-4875-9cf1-fcbd4d9b484f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logging_step(iter_num, losses, lr, running_mfu, best_val_loss):\n",
    "    print(f\"step {iter_num}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    wandb.log({\n",
    "        \"iter\": iter_num,\n",
    "        \"train/loss\": losses['train'],\n",
    "        \"val/loss\": losses['val'],\n",
    "        \"lr\": lr,\n",
    "        \"mfu\": running_mfu*100,\n",
    "    })\n",
    "\n",
    "    if losses['val'] < best_val_loss:\n",
    "        best_val_loss = losses['val']\n",
    "        if iter_num > 0:\n",
    "            checkpoint = {\n",
    "                'model': model.state_dict(),\n",
    "                'optimizer': optimizer.state_dict(),\n",
    "                'model_args': model_args,\n",
    "                'iter_num': iter_num,\n",
    "                'best_val_loss': best_val_loss,\n",
    "                'config': config,\n",
    "            }\n",
    "            torch.save(checkpoint, os.path.join(out_dir, f'ckpt_{iter_num}.pt'))\n",
    "\n",
    "    return best_val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "72d0e86e-1493-483b-a9ca-049548a71b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    \n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            with ctx:\n",
    "                logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "        \n",
    "    model.train()\n",
    "    \n",
    "    return out\n",
    "\n",
    "# TODO: подсчитать метрики!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d4dd85f3-4cb7-4d5a-b26e-3e005e4458d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(iter_num=0, best_val_loss=1e9, local_iter_num=0, running_mfu=-1.0):\n",
    "    t0 = time.time()\n",
    "    X, Y = get_batch('train')\n",
    "    \n",
    "    while True:\n",
    "        lr = get_lr(iter_num)\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "    \n",
    "        if iter_num % eval_interval == 0:\n",
    "            losses = estimate_loss()\n",
    "            best_val_loss = logging_step(iter_num, losses, lr, running_mfu, best_val_loss)\n",
    "                    \n",
    "        for micro_step in range(gradient_accumulation_steps):\n",
    "            with ctx:\n",
    "                logits, loss = model(X, Y)\n",
    "                loss = loss / gradient_accumulation_steps\n",
    "                \n",
    "            X, Y = get_batch('train')\n",
    "            scaler.scale(loss).backward()\n",
    "\n",
    "        if grad_clip != 0.0:\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "    \n",
    "        t1 = time.time()\n",
    "        dt = t1 - t0\n",
    "        t0 = t1\n",
    "        if iter_num % log_interval == 0:\n",
    "            lossf = loss.item() * gradient_accumulation_steps\n",
    "            \n",
    "            if local_iter_num >= 5:\n",
    "                mfu = model.estimate_mfu(config.batch_size * gradient_accumulation_steps, dt)\n",
    "                running_mfu = mfu if running_mfu == -1.0 else 0.9*running_mfu + 0.1*mfu\n",
    "            print(f\"iter {iter_num}: loss {lossf:.4f}, time {dt*1000:.2f}ms, mfu {running_mfu*100:.2f}%\")\n",
    "            \n",
    "        iter_num += 1\n",
    "        local_iter_num += 1\n",
    "    \n",
    "        if iter_num > max_iters:\n",
    "            return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a29302-abdf-4e60-a58d-03f10706890e",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 23.8704, val loss 23.8864\n",
      "iter 0: loss 23.7604, time 79314.08ms, mfu -100.00%\n",
      "iter 10: loss 8.3796, time 806.84ms, mfu 57.64%\n",
      "iter 20: loss 7.0542, time 807.99ms, mfu 57.63%\n",
      "iter 30: loss 6.7136, time 811.42ms, mfu 57.60%\n",
      "iter 40: loss 6.1736, time 815.15ms, mfu 57.54%\n",
      "iter 50: loss 5.9476, time 814.46ms, mfu 57.50%\n",
      "iter 60: loss 5.5028, time 814.88ms, mfu 57.46%\n",
      "iter 70: loss 5.3241, time 817.85ms, mfu 57.40%\n",
      "iter 80: loss 5.3117, time 821.37ms, mfu 57.32%\n",
      "iter 90: loss 5.1199, time 820.81ms, mfu 57.25%\n",
      "iter 100: loss 4.8632, time 820.27ms, mfu 57.20%\n",
      "iter 110: loss 4.8476, time 816.96ms, mfu 57.17%\n",
      "iter 120: loss 4.9380, time 820.66ms, mfu 57.12%\n",
      "iter 130: loss 4.8123, time 818.94ms, mfu 57.09%\n",
      "iter 140: loss 4.7267, time 825.71ms, mfu 57.01%\n",
      "iter 150: loss 4.7436, time 821.95ms, mfu 56.97%\n",
      "iter 160: loss 4.6049, time 821.90ms, mfu 56.93%\n",
      "iter 170: loss 4.6648, time 827.01ms, mfu 56.86%\n",
      "iter 180: loss 4.5481, time 823.54ms, mfu 56.82%\n",
      "iter 190: loss 4.4443, time 823.36ms, mfu 56.79%\n",
      "iter 200: loss 4.3611, time 823.44ms, mfu 56.76%\n",
      "iter 210: loss 4.4820, time 822.38ms, mfu 56.73%\n",
      "iter 220: loss 4.2835, time 823.75ms, mfu 56.71%\n",
      "iter 230: loss 4.4964, time 821.20ms, mfu 56.70%\n",
      "iter 240: loss 4.3302, time 821.45ms, mfu 56.69%\n",
      "step 250: train loss 4.1906, val loss 4.2417\n",
      "iter 250: loss 4.2045, time 85379.62ms, mfu 51.08%\n",
      "iter 260: loss 4.2499, time 807.97ms, mfu 51.72%\n",
      "iter 270: loss 4.2911, time 808.87ms, mfu 52.30%\n",
      "iter 280: loss 4.3280, time 812.74ms, mfu 52.79%\n",
      "iter 290: loss 4.0958, time 815.66ms, mfu 53.22%\n",
      "iter 300: loss 4.1985, time 814.29ms, mfu 53.60%\n",
      "iter 310: loss 4.1835, time 816.21ms, mfu 53.94%\n",
      "iter 320: loss 4.2346, time 818.99ms, mfu 54.23%\n",
      "iter 330: loss 4.1151, time 818.07ms, mfu 54.49%\n",
      "iter 340: loss 3.9457, time 816.60ms, mfu 54.73%\n",
      "iter 350: loss 4.1598, time 822.03ms, mfu 54.92%\n",
      "iter 360: loss 4.1303, time 826.53ms, mfu 55.05%\n",
      "iter 370: loss 4.1597, time 819.86ms, mfu 55.22%\n",
      "iter 380: loss 4.2844, time 820.97ms, mfu 55.36%\n",
      "iter 390: loss 3.9942, time 826.39ms, mfu 55.45%\n",
      "iter 400: loss 4.0609, time 823.32ms, mfu 55.56%\n",
      "iter 410: loss 3.9040, time 819.86ms, mfu 55.67%\n",
      "iter 420: loss 4.0243, time 823.69ms, mfu 55.75%\n",
      "iter 430: loss 4.0298, time 821.03ms, mfu 55.84%\n",
      "iter 440: loss 4.1033, time 822.06ms, mfu 55.91%\n",
      "iter 450: loss 3.7854, time 822.58ms, mfu 55.98%\n",
      "iter 460: loss 3.7919, time 822.04ms, mfu 56.04%\n",
      "iter 470: loss 3.9846, time 824.44ms, mfu 56.07%\n",
      "iter 480: loss 3.7792, time 823.84ms, mfu 56.11%\n",
      "iter 490: loss 3.8350, time 824.82ms, mfu 56.14%\n",
      "step 500: train loss 3.7110, val loss 3.7730\n",
      "iter 500: loss 3.9914, time 85728.35ms, mfu 50.58%\n",
      "iter 510: loss 3.9362, time 806.39ms, mfu 51.29%\n",
      "iter 520: loss 4.0478, time 808.17ms, mfu 51.91%\n",
      "iter 530: loss 3.9144, time 814.70ms, mfu 52.43%\n",
      "iter 540: loss 3.5597, time 813.48ms, mfu 52.90%\n",
      "iter 550: loss 3.6690, time 810.07ms, mfu 53.35%\n",
      "iter 560: loss 3.8316, time 814.84ms, mfu 53.73%\n",
      "iter 570: loss 3.6979, time 814.16ms, mfu 54.07%\n",
      "iter 580: loss 3.9051, time 816.80ms, mfu 54.35%\n",
      "iter 590: loss 3.6524, time 820.05ms, mfu 54.59%\n",
      "iter 600: loss 3.5838, time 823.34ms, mfu 54.78%\n",
      "iter 610: loss 3.5676, time 816.34ms, mfu 55.00%\n",
      "iter 620: loss 3.6597, time 819.21ms, mfu 55.17%\n",
      "iter 630: loss 3.4500, time 821.73ms, mfu 55.32%\n",
      "iter 640: loss 3.4778, time 820.80ms, mfu 55.45%\n",
      "iter 650: loss 3.5880, time 818.38ms, mfu 55.59%\n",
      "iter 660: loss 3.5825, time 819.45ms, mfu 55.70%\n",
      "iter 670: loss 3.4978, time 825.85ms, mfu 55.76%\n",
      "iter 680: loss 3.4832, time 821.80ms, mfu 55.85%\n",
      "iter 690: loss 3.5070, time 821.96ms, mfu 55.92%\n",
      "iter 700: loss 3.5888, time 825.49ms, mfu 55.96%\n",
      "iter 710: loss 3.6248, time 823.92ms, mfu 56.01%\n",
      "iter 720: loss 3.6036, time 819.05ms, mfu 56.09%\n",
      "iter 730: loss 3.7862, time 822.91ms, mfu 56.13%\n",
      "iter 740: loss 3.6003, time 825.48ms, mfu 56.15%\n",
      "step 750: train loss 3.3282, val loss 3.4324\n",
      "iter 750: loss 3.5267, time 64215.72ms, mfu 50.61%\n",
      "iter 760: loss 3.5564, time 814.48ms, mfu 51.26%\n",
      "iter 770: loss 3.5839, time 815.99ms, mfu 51.83%\n",
      "iter 780: loss 3.5270, time 820.07ms, mfu 52.32%\n",
      "iter 790: loss 3.2418, time 827.56ms, mfu 52.71%\n",
      "iter 800: loss 3.3230, time 819.65ms, mfu 53.11%\n",
      "iter 810: loss 3.3104, time 820.47ms, mfu 53.47%\n",
      "iter 820: loss 3.2627, time 821.54ms, mfu 53.78%\n",
      "iter 830: loss 3.3765, time 820.30ms, mfu 54.07%\n",
      "iter 840: loss 3.5575, time 821.18ms, mfu 54.33%\n",
      "iter 850: loss 3.3505, time 818.96ms, mfu 54.57%\n",
      "iter 860: loss 3.3698, time 821.06ms, mfu 54.78%\n",
      "iter 870: loss 3.5711, time 820.85ms, mfu 54.97%\n",
      "iter 880: loss 3.2874, time 818.81ms, mfu 55.15%\n",
      "iter 890: loss 3.3766, time 822.77ms, mfu 55.29%\n",
      "iter 900: loss 3.2804, time 823.68ms, mfu 55.40%\n",
      "iter 910: loss 3.4331, time 826.23ms, mfu 55.49%\n",
      "iter 920: loss 2.9625, time 821.87ms, mfu 55.60%\n",
      "iter 930: loss 3.2372, time 824.00ms, mfu 55.69%\n",
      "iter 940: loss 3.3308, time 821.71ms, mfu 55.78%\n",
      "iter 950: loss 3.2388, time 823.45ms, mfu 55.85%\n",
      "iter 960: loss 3.1580, time 820.54ms, mfu 55.93%\n",
      "iter 970: loss 3.3561, time 824.28ms, mfu 55.98%\n",
      "iter 980: loss 3.2434, time 822.14ms, mfu 56.04%\n",
      "iter 990: loss 3.2884, time 823.73ms, mfu 56.08%\n",
      "step 1000: train loss 3.1048, val loss 3.2701\n",
      "iter 1000: loss 3.4094, time 64240.11ms, mfu 50.54%\n",
      "iter 1010: loss 3.0716, time 810.91ms, mfu 51.22%\n",
      "iter 1020: loss 3.2167, time 814.81ms, mfu 51.81%\n",
      "iter 1030: loss 3.0544, time 818.29ms, mfu 52.31%\n",
      "iter 1040: loss 3.3248, time 817.52ms, mfu 52.77%\n",
      "iter 1050: loss 3.1338, time 823.69ms, mfu 53.14%\n",
      "iter 1060: loss 3.1308, time 819.73ms, mfu 53.50%\n",
      "iter 1070: loss 2.8843, time 820.92ms, mfu 53.81%\n",
      "iter 1080: loss 3.1898, time 819.93ms, mfu 54.10%\n",
      "iter 1090: loss 3.0023, time 817.00ms, mfu 54.39%\n",
      "iter 1100: loss 3.2119, time 818.08ms, mfu 54.63%\n",
      "iter 1110: loss 3.0809, time 817.48ms, mfu 54.86%\n",
      "iter 1120: loss 3.2991, time 817.91ms, mfu 55.06%\n",
      "iter 1130: loss 3.2954, time 816.32ms, mfu 55.25%\n",
      "iter 1140: loss 3.2511, time 818.41ms, mfu 55.41%\n",
      "iter 1150: loss 3.0432, time 822.17ms, mfu 55.52%\n",
      "iter 1160: loss 3.1181, time 822.24ms, mfu 55.63%\n",
      "iter 1170: loss 3.2848, time 819.09ms, mfu 55.74%\n",
      "iter 1180: loss 2.9762, time 822.92ms, mfu 55.82%\n",
      "iter 1190: loss 3.1760, time 825.06ms, mfu 55.87%\n",
      "iter 1200: loss 3.2363, time 821.59ms, mfu 55.95%\n",
      "iter 1210: loss 3.0644, time 818.45ms, mfu 56.03%\n",
      "iter 1220: loss 3.1747, time 821.94ms, mfu 56.09%\n",
      "iter 1230: loss 2.9022, time 819.82ms, mfu 56.15%\n",
      "iter 1240: loss 3.0333, time 826.26ms, mfu 56.16%\n",
      "step 1250: train loss 2.9432, val loss 3.1112\n",
      "iter 1250: loss 2.9361, time 64082.24ms, mfu 50.62%\n",
      "iter 1260: loss 2.9155, time 810.83ms, mfu 51.29%\n",
      "iter 1270: loss 2.8771, time 813.92ms, mfu 51.88%\n",
      "iter 1280: loss 3.2413, time 819.42ms, mfu 52.37%\n",
      "iter 1290: loss 2.9540, time 817.10ms, mfu 52.82%\n",
      "iter 1300: loss 3.0799, time 820.48ms, mfu 53.21%\n",
      "iter 1310: loss 3.1036, time 819.12ms, mfu 53.56%\n",
      "iter 1320: loss 2.9993, time 817.03ms, mfu 53.90%\n",
      "iter 1330: loss 2.8628, time 820.79ms, mfu 54.18%\n",
      "iter 1340: loss 3.0900, time 816.87ms, mfu 54.45%\n",
      "iter 1350: loss 3.0336, time 818.54ms, mfu 54.69%\n",
      "iter 1360: loss 3.2907, time 818.10ms, mfu 54.90%\n",
      "iter 1370: loss 3.1287, time 816.81ms, mfu 55.11%\n",
      "iter 1380: loss 3.1124, time 821.18ms, mfu 55.26%\n",
      "iter 1390: loss 2.9968, time 816.47ms, mfu 55.43%\n",
      "iter 1400: loss 2.8424, time 816.66ms, mfu 55.58%\n",
      "iter 1410: loss 3.1130, time 819.54ms, mfu 55.70%\n",
      "iter 1420: loss 3.0116, time 820.65ms, mfu 55.79%\n",
      "iter 1430: loss 3.3654, time 821.11ms, mfu 55.88%\n",
      "iter 1440: loss 2.8948, time 820.87ms, mfu 55.96%\n",
      "iter 1450: loss 3.0290, time 818.23ms, mfu 56.04%\n",
      "iter 1460: loss 2.8866, time 816.85ms, mfu 56.13%\n",
      "iter 1470: loss 2.8543, time 819.17ms, mfu 56.20%\n",
      "iter 1480: loss 2.8538, time 815.95ms, mfu 56.28%\n",
      "iter 1490: loss 2.9547, time 817.63ms, mfu 56.34%\n",
      "step 1500: train loss 2.8145, val loss 3.0117\n",
      "iter 1500: loss 2.9123, time 63807.22ms, mfu 50.78%\n",
      "iter 1510: loss 2.7949, time 815.03ms, mfu 51.40%\n",
      "iter 1520: loss 2.9063, time 813.83ms, mfu 51.98%\n",
      "iter 1530: loss 3.0255, time 817.37ms, mfu 52.47%\n",
      "iter 1540: loss 2.9938, time 815.18ms, mfu 52.93%\n",
      "iter 1550: loss 2.6840, time 817.15ms, mfu 53.33%\n",
      "iter 1560: loss 2.9129, time 818.78ms, mfu 53.67%\n",
      "iter 1570: loss 2.9629, time 819.76ms, mfu 53.98%\n",
      "iter 1580: loss 2.9934, time 819.73ms, mfu 54.25%\n",
      "iter 1590: loss 2.8840, time 814.84ms, mfu 54.54%\n",
      "iter 1600: loss 2.9298, time 814.35ms, mfu 54.79%\n",
      "iter 1610: loss 3.0163, time 816.38ms, mfu 55.01%\n",
      "iter 1620: loss 3.3891, time 818.62ms, mfu 55.19%\n",
      "iter 1630: loss 2.9448, time 815.88ms, mfu 55.37%\n",
      "iter 1640: loss 2.8063, time 817.55ms, mfu 55.52%\n",
      "iter 1650: loss 2.8307, time 816.58ms, mfu 55.67%\n",
      "iter 1660: loss 2.9952, time 817.53ms, mfu 55.79%\n",
      "iter 1670: loss 2.7669, time 818.14ms, mfu 55.89%\n",
      "iter 1680: loss 2.8013, time 819.71ms, mfu 55.98%\n",
      "iter 1690: loss 2.8442, time 816.51ms, mfu 56.07%\n",
      "iter 1700: loss 3.0474, time 814.03ms, mfu 56.18%\n",
      "iter 1710: loss 2.9979, time 823.53ms, mfu 56.21%\n",
      "iter 1720: loss 2.9407, time 815.77ms, mfu 56.29%\n",
      "iter 1730: loss 2.9901, time 817.62ms, mfu 56.35%\n",
      "iter 1740: loss 2.8352, time 819.09ms, mfu 56.39%\n",
      "step 1750: train loss 2.7181, val loss 2.9521\n",
      "iter 1750: loss 2.8934, time 64113.48ms, mfu 50.82%\n",
      "iter 1760: loss 2.7983, time 811.25ms, mfu 51.47%\n",
      "iter 1770: loss 2.8779, time 814.67ms, mfu 52.04%\n",
      "iter 1780: loss 2.8755, time 815.40ms, mfu 52.54%\n",
      "iter 1790: loss 2.8891, time 817.43ms, mfu 52.97%\n",
      "iter 1800: loss 2.8416, time 820.54ms, mfu 53.34%\n",
      "iter 1810: loss 2.7840, time 823.55ms, mfu 53.65%\n",
      "iter 1820: loss 2.8085, time 814.40ms, mfu 54.00%\n",
      "iter 1830: loss 2.7628, time 812.75ms, mfu 54.32%\n",
      "iter 1840: loss 2.6072, time 815.01ms, mfu 54.59%\n",
      "iter 1850: loss 2.6500, time 815.00ms, mfu 54.84%\n",
      "iter 1860: loss 2.7448, time 817.63ms, mfu 55.05%\n",
      "iter 1870: loss 2.7759, time 811.97ms, mfu 55.27%\n",
      "iter 1880: loss 2.8551, time 816.24ms, mfu 55.44%\n",
      "iter 1890: loss 2.7655, time 815.51ms, mfu 55.60%\n",
      "iter 1900: loss 2.7609, time 815.84ms, mfu 55.74%\n",
      "iter 1910: loss 2.7223, time 816.24ms, mfu 55.86%\n",
      "iter 1920: loss 2.8226, time 817.95ms, mfu 55.96%\n",
      "iter 1930: loss 2.8053, time 815.33ms, mfu 56.07%\n",
      "iter 1940: loss 2.7754, time 816.71ms, mfu 56.16%\n",
      "iter 1950: loss 2.9101, time 816.12ms, mfu 56.24%\n",
      "iter 1960: loss 2.8216, time 817.85ms, mfu 56.30%\n",
      "iter 1970: loss 2.8632, time 818.26ms, mfu 56.35%\n",
      "iter 1980: loss 2.8674, time 817.70ms, mfu 56.41%\n",
      "iter 1990: loss 2.7144, time 819.73ms, mfu 56.44%\n"
     ]
    }
   ],
   "source": [
    "train_loop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea1b94c-0850-4cfc-9627-381ac22e96c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b3f1cc-5428-4601-ab2d-08a398fcc624",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c21a64-c08a-49ba-b195-005bbf57ff17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4993c2a4-13c9-4dd6-8f0e-f231a7890318",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7acf002-6ed9-446f-a615-f24ab934d3af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "main_venv",
   "language": "python",
   "name": ".main_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
