{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d0cc3f9-0c2d-4e7d-bae4-19132103b9d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import math\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "from dataclasses import dataclass\n",
    "import wandb\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.amp import autocast, GradScaler\n",
    "\n",
    "from reflex_model import GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9dd4553-914b-4969-8363-b4cb98cced3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch._dynamo\n",
    "torch._dynamo.config.suppress_errors = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "91a5eb4e-d5b5-4c74-9271-307ddee0761d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class GPTConfig:\n",
    "    batch_size: int = 8\n",
    "    block_size: int = 2048\n",
    "    vocab_size: int = 50257\n",
    "    n_layer: int = 6\n",
    "    n_head: int = 8\n",
    "    n_embd: int = 1536\n",
    "    dropout: float = 0.1\n",
    "    bias: bool = True\n",
    "    pretrained_model_path: str=\"/home/user/models/rugpt\"\n",
    "    local_files_only: bool=True\n",
    "    \n",
    "config = GPTConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb10d596-8225-4ec5-a5e6-22563e99f148",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = json.load(open('/home/user/DecoderArchitecture/run_params.json'))\n",
    "\n",
    "params['wandb_run_name'] = f\"KornilovaK-{params['wandb_project']}\"\n",
    "params['dtype'] = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16'\n",
    "os.makedirs(params['out_dir'], exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5f49362b-5ae8-4f08-9dea-f4b9a40b0445",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens per iteration will be: 16,384\n"
     ]
    }
   ],
   "source": [
    "tokens_per_iter = params['gradient_accumulation_steps'] * params['ddp_world_size'] * config.batch_size * config.block_size\n",
    "print(f\"tokens per iteration will be: {tokens_per_iter:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c2fe98c1-2878-4ac4-9620-c672443e1234",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1337 + params['seed_offset'])\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "device_type = params['device']\n",
    "\n",
    "ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[params['dtype']]\n",
    "ctx = autocast(device_type=device_type, dtype=ptdtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "101f57d4-abcb-46a9-b93b-64403fdd29a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_args = dict(\n",
    "    n_layer=config.n_layer,\n",
    "    n_head=config.n_head,\n",
    "    n_embd=config.n_embd,\n",
    "    block_size=config.block_size,\n",
    "    bias=config.bias,\n",
    "    vocab_size=config.vocab_size,\n",
    "    dropout=config.dropout\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f8beaa3d-dee9-4dd0-95c1-59a40ba2e5e1",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copied: transformer.wte.weight -> transformer.wte.weight\n",
      "Copied: lm_head.weight -> lm_head.weight\n",
      "Copied: transformer.wpe.weight -> transformer.wpe.weight\n",
      "Copied: transformer.ln_f.weight -> transformer.ln_f.weight\n",
      "Copied: transformer.ln_f.bias -> transformer.ln_f.bias\n",
      "Copied: transformer.h.0.ln_1.weight -> transformer.h.0.ln_1.weight\n",
      "Copied: transformer.h.0.ln_1.bias -> transformer.h.0.ln_1.bias\n",
      "Copied: transformer.h.0.ln_2.weight -> transformer.h.0.ln_2.weight\n",
      "Copied: transformer.h.0.ln_2.bias -> transformer.h.0.ln_2.bias\n",
      "Copied: transformer.h.0.attn.c_attn.weight -> transformer.h.0.attn.c_attn.weight\n",
      "Copied: transformer.h.0.attn.c_attn.bias -> transformer.h.0.attn.c_attn.bias\n",
      "Copied: transformer.h.0.attn.c_proj.weight -> transformer.h.0.attn.c_proj.weight\n",
      "Copied: transformer.h.0.attn.c_proj.bias -> transformer.h.0.attn.c_proj.bias\n",
      "Copied: transformer.h.0.mlp.c_fc.weight -> transformer.h.0.mlp.c_fc.weight\n",
      "Copied: transformer.h.0.mlp.c_fc.bias -> transformer.h.0.mlp.c_fc.bias\n",
      "Copied: transformer.h.0.mlp.c_proj.weight -> transformer.h.0.mlp.c_proj.weight\n",
      "Copied: transformer.h.0.mlp.c_proj.bias -> transformer.h.0.mlp.c_proj.bias\n",
      "Copied: transformer.h.1.ln_1.weight -> transformer.h.1.ln_1.weight\n",
      "Copied: transformer.h.1.ln_1.bias -> transformer.h.1.ln_1.bias\n",
      "Copied: transformer.h.1.ln_2.weight -> transformer.h.1.ln_2.weight\n",
      "Copied: transformer.h.1.ln_2.bias -> transformer.h.1.ln_2.bias\n",
      "Copied: transformer.h.1.attn.c_attn.weight -> transformer.h.1.attn.c_attn.weight\n",
      "Copied: transformer.h.1.attn.c_attn.bias -> transformer.h.1.attn.c_attn.bias\n",
      "Copied: transformer.h.1.attn.c_proj.weight -> transformer.h.1.attn.c_proj.weight\n",
      "Copied: transformer.h.1.attn.c_proj.bias -> transformer.h.1.attn.c_proj.bias\n",
      "Copied: transformer.h.1.mlp.c_fc.weight -> transformer.h.1.mlp.c_fc.weight\n",
      "Copied: transformer.h.1.mlp.c_fc.bias -> transformer.h.1.mlp.c_fc.bias\n",
      "Copied: transformer.h.1.mlp.c_proj.weight -> transformer.h.1.mlp.c_proj.weight\n",
      "Copied: transformer.h.1.mlp.c_proj.bias -> transformer.h.1.mlp.c_proj.bias\n",
      "Copied: transformer.h.2.ln_1.weight -> transformer.h.2.ln_1.weight\n",
      "Copied: transformer.h.2.ln_1.bias -> transformer.h.2.ln_1.bias\n",
      "Copied: transformer.h.2.ln_2.weight -> transformer.h.2.ln_2.weight\n",
      "Copied: transformer.h.2.ln_2.bias -> transformer.h.2.ln_2.bias\n",
      "Copied: transformer.h.2.attn.c_attn.weight -> transformer.h.2.attn.c_attn.weight\n",
      "Copied: transformer.h.2.attn.c_attn.bias -> transformer.h.2.attn.c_attn.bias\n",
      "Copied: transformer.h.2.attn.c_proj.weight -> transformer.h.2.attn.c_proj.weight\n",
      "Copied: transformer.h.2.attn.c_proj.bias -> transformer.h.2.attn.c_proj.bias\n",
      "Copied: transformer.h.2.mlp.c_fc.weight -> transformer.h.2.mlp.c_fc.weight\n",
      "Copied: transformer.h.2.mlp.c_fc.bias -> transformer.h.2.mlp.c_fc.bias\n",
      "Copied: transformer.h.2.mlp.c_proj.weight -> transformer.h.2.mlp.c_proj.weight\n",
      "Copied: transformer.h.2.mlp.c_proj.bias -> transformer.h.2.mlp.c_proj.bias\n",
      "Copied: transformer.h.3.ln_1.weight -> transformer.h.3.ln_1.weight\n",
      "Copied: transformer.h.3.ln_1.bias -> transformer.h.3.ln_1.bias\n",
      "Copied: transformer.h.3.ln_2.weight -> transformer.h.3.ln_2.weight\n",
      "Copied: transformer.h.3.ln_2.bias -> transformer.h.3.ln_2.bias\n",
      "Copied: transformer.h.3.attn.c_attn.weight -> transformer.h.3.attn.c_attn.weight\n",
      "Copied: transformer.h.3.attn.c_attn.bias -> transformer.h.3.attn.c_attn.bias\n",
      "Copied: transformer.h.3.attn.c_proj.weight -> transformer.h.3.attn.c_proj.weight\n",
      "Copied: transformer.h.3.attn.c_proj.bias -> transformer.h.3.attn.c_proj.bias\n",
      "Copied: transformer.h.3.mlp.c_fc.weight -> transformer.h.3.mlp.c_fc.weight\n",
      "Copied: transformer.h.3.mlp.c_fc.bias -> transformer.h.3.mlp.c_fc.bias\n",
      "Copied: transformer.h.3.mlp.c_proj.weight -> transformer.h.3.mlp.c_proj.weight\n",
      "Copied: transformer.h.3.mlp.c_proj.bias -> transformer.h.3.mlp.c_proj.bias\n",
      "Copied: transformer.h.4.ln_1.weight -> transformer.h.4.ln_1.weight\n",
      "Copied: transformer.h.4.ln_1.bias -> transformer.h.4.ln_1.bias\n",
      "Copied: transformer.h.4.ln_2.weight -> transformer.h.4.ln_2.weight\n",
      "Copied: transformer.h.4.ln_2.bias -> transformer.h.4.ln_2.bias\n",
      "Copied: transformer.h.4.attn.c_attn.weight -> transformer.h.4.attn.c_attn.weight\n",
      "Copied: transformer.h.4.attn.c_attn.bias -> transformer.h.4.attn.c_attn.bias\n",
      "Copied: transformer.h.4.attn.c_proj.weight -> transformer.h.4.attn.c_proj.weight\n",
      "Copied: transformer.h.4.attn.c_proj.bias -> transformer.h.4.attn.c_proj.bias\n",
      "Copied: transformer.h.4.mlp.c_fc.weight -> transformer.h.4.mlp.c_fc.weight\n",
      "Copied: transformer.h.4.mlp.c_fc.bias -> transformer.h.4.mlp.c_fc.bias\n",
      "Copied: transformer.h.4.mlp.c_proj.weight -> transformer.h.4.mlp.c_proj.weight\n",
      "Copied: transformer.h.4.mlp.c_proj.bias -> transformer.h.4.mlp.c_proj.bias\n",
      "Copied: transformer.h.5.ln_1.weight -> transformer.h.5.ln_1.weight\n",
      "Copied: transformer.h.5.ln_1.bias -> transformer.h.5.ln_1.bias\n",
      "Copied: transformer.h.5.ln_2.weight -> transformer.h.5.ln_2.weight\n",
      "Copied: transformer.h.5.ln_2.bias -> transformer.h.5.ln_2.bias\n",
      "Copied: transformer.h.5.attn.c_attn.weight -> transformer.h.5.attn.c_attn.weight\n",
      "Copied: transformer.h.5.attn.c_attn.bias -> transformer.h.5.attn.c_attn.bias\n",
      "Copied: transformer.h.5.attn.c_proj.weight -> transformer.h.5.attn.c_proj.weight\n",
      "Copied: transformer.h.5.attn.c_proj.bias -> transformer.h.5.attn.c_proj.bias\n",
      "Copied: transformer.h.5.mlp.c_fc.weight -> transformer.h.5.mlp.c_fc.weight\n",
      "Copied: transformer.h.5.mlp.c_fc.bias -> transformer.h.5.mlp.c_fc.bias\n",
      "Copied: transformer.h.5.mlp.c_proj.weight -> transformer.h.5.mlp.c_proj.weight\n",
      "Copied: transformer.h.5.mlp.c_proj.bias -> transformer.h.5.mlp.c_proj.bias\n",
      "Initializing new parameter: transformer.h.0.mlp.act.proj.weight\n",
      "Initializing new parameter: transformer.h.0.mlp.act.proj.bias\n",
      "Initializing new parameter: transformer.h.1.mlp.act.proj.weight\n",
      "Initializing new parameter: transformer.h.1.mlp.act.proj.bias\n",
      "Initializing new parameter: transformer.h.2.mlp.act.proj.weight\n",
      "Initializing new parameter: transformer.h.2.mlp.act.proj.bias\n",
      "Initializing new parameter: transformer.h.3.mlp.act.proj.weight\n",
      "Initializing new parameter: transformer.h.3.mlp.act.proj.bias\n",
      "Initializing new parameter: transformer.h.4.mlp.act.proj.weight\n",
      "Initializing new parameter: transformer.h.4.mlp.act.proj.bias\n",
      "Initializing new parameter: transformer.h.5.mlp.act.proj.weight\n",
      "Initializing new parameter: transformer.h.5.mlp.act.proj.bias\n",
      "est checkpoint size: 8.40 GB\n",
      "700.2 millions of params\n"
     ]
    }
   ],
   "source": [
    "scaler = GradScaler()\n",
    "\n",
    "model = GPT(config)\n",
    "\n",
    "params_total = model.get_num_params()\n",
    "params_bytes = params_total*4\n",
    "params_and_buffers_bytes = params_bytes + 2*params_bytes\n",
    "print(f\"est checkpoint size: {params_and_buffers_bytes/1e9:.2f} GB\")\n",
    "print(f\"{params_total/1e6:.1f} millions of params\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9a493cb5-72e0-446d-bcc5-c503295bd897",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num decayed parameter tensors: 32, with 703,194,624 parameters\n",
      "num non-decayed parameter tensors: 56, with 196,608 parameters\n",
      "using fused AdamW: True\n"
     ]
    }
   ],
   "source": [
    "optimizer = model.configure_optimizers(params['weight_decay'], params['learning_rate'], (params['beta1'], params['beta2']), device_type)\n",
    "checkpoint = None\n",
    "\n",
    "model = torch.compile(model.to(params['device']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9ddee2a4-4203-4079-a118-c141bacc64e6",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Failed to detect the name of this notebook. You can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkornilova_eka\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.20.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/user/DecoderArchitecture/wandb/run-20250614_161848-l5bk4e1j</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/kornilova_eka/reflex_attention/runs/l5bk4e1j' target=\"_blank\">KornilovaK-reflex_attention</a></strong> to <a href='https://wandb.ai/kornilova_eka/reflex_attention' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/kornilova_eka/reflex_attention' target=\"_blank\">https://wandb.ai/kornilova_eka/reflex_attention</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/kornilova_eka/reflex_attention/runs/l5bk4e1j' target=\"_blank\">https://wandb.ai/kornilova_eka/reflex_attention/runs/l5bk4e1j</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/kornilova_eka/reflex_attention/runs/l5bk4e1j?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7fc344c90b50>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(project=params['wandb_project'], name=params['wandb_run_name'], config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7c8430ca-c7b1-4998-bac3-5169f9b04ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size                    = config.block_size\n",
    "batch_size                    = config.batch_size\n",
    "data_dir                      = params['data_dir']\n",
    "learning_rate                 = params['learning_rate']\n",
    "warmup_iters                  = params['warmup_iters']\n",
    "out_dir                       = params['out_dir']\n",
    "log_interval                  = params['log_interval']\n",
    "max_iters                     = params['max_iters']\n",
    "gradient_accumulation_steps   = params['gradient_accumulation_steps']\n",
    "grad_clip                     = params['grad_clip']\n",
    "eval_interval                 = params['eval_interval']\n",
    "eval_iters                    = params['eval_iters']\n",
    "min_lr                        = params['min_lr']\n",
    "lr_decay_iters                = params['lr_decay_iters']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9291b651-21f8-4f19-bb75-4afede72a7e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lr(it):\n",
    "    if it < warmup_iters:\n",
    "        return learning_rate * it / warmup_iters\n",
    "    if it > lr_decay_iters:\n",
    "        return min_lr\n",
    "        \n",
    "    decay_ratio = (it - warmup_iters) / (lr_decay_iters - warmup_iters)\n",
    "    assert 0 <= decay_ratio <= 1\n",
    "    \n",
    "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio))\n",
    "    return min_lr + coeff * (learning_rate - min_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a4fe7cec-4765-40c0-af02-68a6aca9faa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(split):\n",
    "    if split == 'train':\n",
    "        data = np.memmap(os.path.join(data_dir, 'train.bin'), dtype=np.uint16, mode='r')\n",
    "    else:\n",
    "        data = np.memmap(os.path.join(data_dir, 'val.bin'), dtype=np.uint16, mode='r')\n",
    "        \n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([torch.from_numpy((data[i:i+block_size]).astype(np.int64)) for i in ix])\n",
    "    y = torch.stack([torch.from_numpy((data[i+1:i+1+block_size]).astype(np.int64)) for i in ix])\n",
    "    \n",
    "    assert device_type == 'cuda'\n",
    "    x, y = x.pin_memory().to(device_type, non_blocking=True), y.pin_memory().to(device_type, non_blocking=True)\n",
    "\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "36e5b9d9-6f55-4875-9cf1-fcbd4d9b484f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logging_step(iter_num, losses, lr, running_mfu, best_val_loss):\n",
    "    print(f\"step {iter_num}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    wandb.log({\n",
    "        \"iter\": iter_num,\n",
    "        \"train/loss\": losses['train'],\n",
    "        \"val/loss\": losses['val'],\n",
    "        \"lr\": lr,\n",
    "        \"mfu\": running_mfu*100,\n",
    "    })\n",
    "\n",
    "    if losses['val'] < best_val_loss:\n",
    "        best_val_loss = losses['val']\n",
    "        if iter_num > 0:\n",
    "            checkpoint = {\n",
    "                'model': model.state_dict(),\n",
    "                'optimizer': optimizer.state_dict(),\n",
    "                'model_args': model_args,\n",
    "                'iter_num': iter_num,\n",
    "                'best_val_loss': best_val_loss,\n",
    "                'config': config,\n",
    "            }\n",
    "            torch.save(checkpoint, os.path.join(out_dir, f'ckpt_{iter_num}.pt'))\n",
    "\n",
    "    return best_val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "72d0e86e-1493-483b-a9ca-049548a71b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    \n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            with ctx:\n",
    "                logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    \n",
    "    return out\n",
    "\n",
    "# TODO: подсчитать метрики!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "fd5b9fb8-2c41-4e57-a4ef-81825508806c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4dd85f3-4cb7-4d5a-b26e-3e005e4458d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(iter_num=0, best_val_loss=1e9, local_iter_num=0, running_mfu=-1.0):\n",
    "    t0 = time.time()\n",
    "    X, Y = get_batch('train')\n",
    "    \n",
    "    while True:\n",
    "        lr = get_lr(iter_num)\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "    \n",
    "        if iter_num % eval_interval == 0:\n",
    "            losses = estimate_loss()\n",
    "            best_val_loss = logging_step(iter_num, losses, lr, running_mfu, best_val_loss)\n",
    "                    \n",
    "        for micro_step in range(gradient_accumulation_steps):\n",
    "            with ctx:\n",
    "                logits, loss = model(X, Y)\n",
    "                loss = loss / gradient_accumulation_steps\n",
    "                \n",
    "            X, Y = get_batch('train')\n",
    "            scaler.scale(loss).backward()\n",
    "\n",
    "        if grad_clip != 0.0:\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "    \n",
    "        t1 = time.time()\n",
    "        dt = t1 - t0\n",
    "        t0 = t1\n",
    "        if iter_num % log_interval == 0:\n",
    "            lossf = loss.item() * gradient_accumulation_steps\n",
    "            \n",
    "            if local_iter_num >= 5:\n",
    "                mfu = model.estimate_mfu(config.batch_size * gradient_accumulation_steps, dt)\n",
    "                running_mfu = mfu if running_mfu == -1.0 else 0.9*running_mfu + 0.1*mfu\n",
    "            print(f\"iter {iter_num}: loss {lossf:.4f}, time {dt*1000:.2f}ms, mfu {running_mfu*100:.2f}%\")\n",
    "            \n",
    "        iter_num += 1\n",
    "        local_iter_num += 1\n",
    "    \n",
    "        if iter_num > max_iters:\n",
    "            return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a29302-abdf-4e60-a58d-03f10706890e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_loop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea1b94c-0850-4cfc-9627-381ac22e96c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "main_venv",
   "language": "python",
   "name": ".main_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
